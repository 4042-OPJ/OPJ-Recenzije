{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Load CSVs\n",
        "train1 = pd.read_csv('train-1.csv')\n",
        "train2 = pd.read_csv('train-2.csv')\n",
        "train3 = pd.read_csv('train-3.csv')\n",
        "TRAIN = pd.concat([train1, train2, train3], ignore_index=True)\n",
        "\n",
        "test1 = pd.read_csv('test-1.csv')\n",
        "test2 = pd.read_csv('test-2.csv')\n",
        "test3 = pd.read_csv('test-3.csv')\n",
        "\n",
        "# Features and labels\n",
        "X_train = TRAIN[\"Sentence\"]\n",
        "y_train = TRAIN[\"Label\"]\n",
        "\n",
        "X_train3 = train3[\"Sentence\"]\n",
        "y_train3 = train3[\"Label\"]\n",
        "\n",
        "X_test1 = test1[\"Sentence\"]\n",
        "y_test1 = test1[\"Label\"]\n",
        "\n",
        "X_test2 = test2[\"Sentence\"]\n",
        "y_test2 = test2[\"Label\"]\n",
        "\n",
        "X_test3 = test3[\"Sentence\"]\n",
        "y_test3 = test3[\"Label\"]"
      ],
      "metadata": {
        "id": "qDZKm4cDyRDl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_tests, y_tests, test_names):\n",
        "    results = []\n",
        "    for X_test, y_test, name in zip(X_tests, y_tests, test_names):\n",
        "        y_pred = model.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results.append({\n",
        "            \"Test Set\": name,\n",
        "            \"Precision\": round(report[\"weighted avg\"][\"precision\"], 4),\n",
        "            \"Recall\": round(report[\"weighted avg\"][\"recall\"], 4),\n",
        "            \"F1-Score\": round(report[\"weighted avg\"][\"f1-score\"], 4),\n",
        "            \"Accuracy\": round(accuracy, 4)\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "PxDEGXhGBAzI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(model_name, results):\n",
        "    print(f\"\\n=== {model_name} Results ===\")\n",
        "    print(\"| Test Set | Precision | Recall | F1-Score | Accuracy |\")\n",
        "    print(\"|----------|-----------|--------|----------|----------|\")\n",
        "    for r in results:\n",
        "        print(f\"| {r['Test Set']} | {r['Precision']} | {r['Recall']} | {r['F1-Score']} | {r['Accuracy']} |\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "51aQ025l_eVw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import traceback\n",
        "\n",
        "# Logistic Regression Pipeline with RandomizedSearchCV\n",
        "logreg_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Define a smaller hyperparameter grid for Logistic Regression\n",
        "logreg_param_dist = {\n",
        "    'clf__C': [0.1, 1, 10],  # Reduced range for C\n",
        "    'clf__solver': ['liblinear', 'saga'],  # Fewer solvers\n",
        "    'clf__penalty': ['l2'],  # 'l1' and 'elasticnet' might not perform well for large data\n",
        "    'clf__max_iter': [1000],  # Reasonable number of iterations\n",
        "    'clf__tol': [1e-4],  # Convergence tolerance\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV for Logistic Regression with error handling\n",
        "logreg_search = RandomizedSearchCV(logreg_pipeline, logreg_param_dist, n_iter=5, cv=2, n_jobs=-1, verbose=1, random_state=42, error_score='raise')\n",
        "try:\n",
        "    logreg_search.fit(X_train, y_train)\n",
        "except Exception as e:\n",
        "    print(f\"Error during Logistic Regression RandomizedSearchCV: {e}\")\n",
        "    # You can also log more information here for debugging, like the problematic parameters\n",
        "    print(\"The error occurred with parameters:\")\n",
        "    print(logreg_search.best_params_ if hasattr(logreg_search, 'best_params_') else \"No best parameters found yet\")\n",
        "    print(\"Stack trace:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# If successful, get the best model and evaluate\n",
        "if 'logreg_search' in locals() and hasattr(logreg_search, 'best_estimator_') and logreg_search.best_estimator_:\n",
        "    logreg_best_model = logreg_search.best_estimator_\n",
        "    logreg_results = evaluate_model(\n",
        "        logreg_best_model,\n",
        "        [X_test1, X_test2, X_test3],\n",
        "        [y_test1, y_test2, y_test3],\n",
        "        [\"Test-1\", \"Test-2\", \"Test-3\"]\n",
        "    )\n",
        "    display_results(\"Logistic Regression\", logreg_results)\n",
        "    print(\"Best hyperparameters for Logistic Regression:\", logreg_search.best_params_)\n",
        "else:\n",
        "    print(\"Logistic Regression RandomizedSearchCV failed. Please check the error messages above.\")\n",
        "\n",
        "# Use SGDClassifier as an alternative for faster performance on large datasets\n",
        "sgd_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', SGDClassifier(loss='log_loss', max_iter=1000, random_state=42))  # Fix 'log' to 'log_loss'\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid for SGDClassifier\n",
        "sgd_param_dist = {\n",
        "    'clf__alpha': np.logspace(-5, 5, 11),  # Regularization strength\n",
        "    'clf__penalty': ['l2', 'elasticnet'],  # Regularization type\n",
        "    'clf__max_iter': [1000, 2000],\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for SGDClassifier with error handling\n",
        "sgd_search = RandomizedSearchCV(sgd_pipeline, sgd_param_dist, n_iter=5, cv=2, n_jobs=-1, verbose=1, random_state=42, error_score='raise')\n",
        "try:\n",
        "    sgd_search.fit(X_train, y_train)\n",
        "except Exception as e:\n",
        "    print(f\"Error during SGDClassifier RandomizedSearchCV: {e}\")\n",
        "    print(\"The error occurred with parameters:\")\n",
        "    print(sgd_search.best_params_ if hasattr(sgd_search, 'best_params_') else \"No best parameters found yet\")\n",
        "    print(\"Stack trace:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# If successful, get the best model and evaluate\n",
        "if 'sgd_search' in locals() and hasattr(sgd_search, 'best_estimator_') and sgd_search.best_estimator_:\n",
        "    sgd_best_model = sgd_search.best_estimator_\n",
        "    sgd_results = evaluate_model(\n",
        "        sgd_best_model,\n",
        "        [X_test1, X_test2, X_test3],\n",
        "        [y_test1, y_test2, y_test3],\n",
        "        [\"Test-1\", \"Test-2\", \"Test-3\"]\n",
        "    )\n",
        "    display_results(\"SGD Classifier\", sgd_results)\n",
        "    print(\"Best hyperparameters for SGD Classifier:\", sgd_search.best_params_)\n",
        "else:\n",
        "    print(\"SGDClassifier RandomizedSearchCV failed. Please check the error messages above.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTZtBsFYEGR9",
        "outputId": "53f898a9-a588-40b7-b3b7-556f83e4c7c7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "\n",
            "=== Logistic Regression Results ===\n",
            "| Test Set | Precision | Recall | F1-Score | Accuracy |\n",
            "|----------|-----------|--------|----------|----------|\n",
            "| Test-1 | 0.3369 | 0.5804 | 0.4263 | 0.5804 |\n",
            "| Test-2 | 0.3383 | 0.5816 | 0.4278 | 0.5816 |\n",
            "| Test-3 | 0.4223 | 0.6499 | 0.512 | 0.6499 |\n",
            "Best hyperparameters for Logistic Regression: {'clf__tol': 0.0001, 'clf__solver': 'liblinear', 'clf__penalty': 'l2', 'clf__max_iter': 1000, 'clf__C': 0.1}\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "\n",
            "=== SGD Classifier Results ===\n",
            "| Test Set | Precision | Recall | F1-Score | Accuracy |\n",
            "|----------|-----------|--------|----------|----------|\n",
            "| Test-1 | 0.3369 | 0.5804 | 0.4263 | 0.5804 |\n",
            "| Test-2 | 0.3383 | 0.5816 | 0.4278 | 0.5816 |\n",
            "| Test-3 | 0.4223 | 0.6499 | 0.512 | 0.6499 |\n",
            "Best hyperparameters for SGD Classifier: {'clf__penalty': 'elasticnet', 'clf__max_iter': 1000, 'clf__alpha': np.float64(10000.0)}\n"
          ]
        }
      ]
    }
  ]
}