{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tFz8NQRi6WW0",
        "outputId": "05b982e5-0a01-479d-d104-297d8a4cf73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN dataset size: 7278\n",
            "Train-3 dataset size: 2448\n",
            "Test-1 dataset size: 653\n",
            "Test-2 dataset size: 741\n",
            "Test-3 dataset size: 774\n",
            "\n",
            "Checking for missing values in datasets:\n",
            "TRAIN missing sentences: 0\n",
            "TRAIN missing labels: 0\n",
            "Train-3 missing sentences: 0\n",
            "Test-1 missing sentences: 0\n",
            "Test-2 missing sentences: 0\n",
            "Test-3 missing sentences: 0\n",
            "\n",
            "Class distribution in TRAIN:\n",
            "Label\n",
            "1    3881\n",
            "0    2003\n",
            "2     790\n",
            "3     452\n",
            "4     152\n",
            "\n",
            "Class distribution in Train-3:\n",
            "Label\n",
            "1    1142\n",
            "0     636\n",
            "2     310\n",
            "3     213\n",
            "4     147\n",
            "\n",
            "==== Evaluating Logistic Regression ====\n",
            "\n",
            "===== Training Logistic Regression on TRAIN (All) on dataset with 7278 samples =====\n",
            "Training time: 17.74 seconds\n",
            "\n",
            "===== Training Logistic Regression on Train-3 on dataset with 2448 samples =====\n",
            "Training time: 2.92 seconds\n",
            "\n",
            "==== Evaluating Random Forest ====\n",
            "\n",
            "===== Training Random Forest on TRAIN (All) on dataset with 7278 samples =====\n",
            "Training time: 55.62 seconds\n",
            "\n",
            "===== Training Random Forest on Train-3 on dataset with 2448 samples =====\n",
            "Training time: 8.29 seconds\n",
            "\n",
            "==== All Evaluation Results ====\n",
            "                                      Training Set Test Set  Precision  Recall     F1  Accuracy Inference Time\n",
            "Logistic Regression on TRAIN (All) on 7278 samples   Test-1     0.5623  0.5528 0.5515    0.5528       0.04 sec\n",
            "Logistic Regression on TRAIN (All) on 7278 samples   Test-2     0.5971  0.5803 0.5882    0.5803       0.04 sec\n",
            "Logistic Regression on TRAIN (All) on 7278 samples   Test-3     0.5679  0.5504 0.5499    0.5504       0.04 sec\n",
            "    Logistic Regression on Train-3 on 2448 samples   Test-1     0.5666  0.3951 0.4490    0.3951       0.03 sec\n",
            "    Logistic Regression on Train-3 on 2448 samples   Test-2     0.6183  0.4197 0.4928    0.4197       0.04 sec\n",
            "    Logistic Regression on Train-3 on 2448 samples   Test-3     0.5924  0.4186 0.4646    0.4186       0.03 sec\n",
            "      Random Forest on TRAIN (All) on 7278 samples   Test-1     0.4626  0.5850 0.4653    0.5850       0.15 sec\n",
            "      Random Forest on TRAIN (All) on 7278 samples   Test-2     0.6841  0.6059 0.4939    0.6059       0.16 sec\n",
            "      Random Forest on TRAIN (All) on 7278 samples   Test-3     0.4950  0.6499 0.5331    0.6499       0.16 sec\n",
            "          Random Forest on Train-3 on 2448 samples   Test-1     0.4519  0.5835 0.4630    0.5835       0.09 sec\n",
            "          Random Forest on Train-3 on 2448 samples   Test-2     0.5146  0.5897 0.4638    0.5897       0.11 sec\n",
            "          Random Forest on Train-3 on 2448 samples   Test-3     0.4908  0.6486 0.5367    0.6486       0.13 sec\n",
            "\n",
            "==== Average F1 Scores by Model and Training Set ====\n",
            "                                      Training Set     F1\n",
            "Logistic Regression on TRAIN (All) on 7278 samples 0.5632\n",
            "      Random Forest on TRAIN (All) on 7278 samples 0.4974\n",
            "          Random Forest on Train-3 on 2448 samples 0.4878\n",
            "    Logistic Regression on Train-3 on 2448 samples 0.4688\n",
            "\n",
            "Best performance: 1    Logistic Regression on TRAIN (All) on 7278 sam...\n",
            "1       Logistic Regression on Train-3 on 2448 samples\n",
            "1         Random Forest on TRAIN (All) on 7278 samples\n",
            "1             Random Forest on Train-3 on 2448 samples\n",
            "Name: Training Set, dtype: object on 1    Test-2\n",
            "1    Test-2\n",
            "1    Test-2\n",
            "1    Test-2\n",
            "Name: Test Set, dtype: object\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to Series.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6b03e32f655d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mbest_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nBest performance: {best_row['Training Set']} on {best_row['Test Set']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Precision: {best_row['Precision']:.4f}, Recall: {best_row['Recall']:.4f}, F1: {best_row['F1']:.4f}, Accuracy: {best_row['Accuracy']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Series.__format__"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load datasets as provided\n",
        "train1 = pd.read_csv('train-1.csv')\n",
        "train2 = pd.read_csv('train-2.csv')\n",
        "train3 = pd.read_csv('train-3.csv')\n",
        "TRAIN = pd.concat([train1, train2, train3], ignore_index=True)\n",
        "test1 = pd.read_csv('test-1.csv')\n",
        "test2 = pd.read_csv('test-2.csv')\n",
        "test3 = pd.read_csv('test-3.csv')\n",
        "\n",
        "# Extract features and labels\n",
        "X_train = TRAIN[\"Sentence\"]\n",
        "y_train = TRAIN[\"Label\"]\n",
        "X_train3 = train3[\"Sentence\"]\n",
        "y_train3 = train3[\"Label\"]\n",
        "X_test1 = test1[\"Sentence\"]\n",
        "y_test1 = test1[\"Label\"]\n",
        "X_test2 = test2[\"Sentence\"]\n",
        "y_test2 = test2[\"Label\"]\n",
        "X_test3 = test3[\"Sentence\"]\n",
        "y_test3 = test3[\"Label\"]\n",
        "\n",
        "print(f\"TRAIN dataset size: {len(TRAIN)}\")\n",
        "print(f\"Train-3 dataset size: {len(train3)}\")\n",
        "print(f\"Test-1 dataset size: {len(test1)}\")\n",
        "print(f\"Test-2 dataset size: {len(test2)}\")\n",
        "print(f\"Test-3 dataset size: {len(test3)}\")\n",
        "\n",
        "# Check for any missing values\n",
        "print(\"\\nChecking for missing values in datasets:\")\n",
        "print(f\"TRAIN missing sentences: {X_train.isna().sum()}\")\n",
        "print(f\"TRAIN missing labels: {y_train.isna().sum()}\")\n",
        "print(f\"Train-3 missing sentences: {X_train3.isna().sum()}\")\n",
        "print(f\"Test-1 missing sentences: {X_test1.isna().sum()}\")\n",
        "print(f\"Test-2 missing sentences: {X_test2.isna().sum()}\")\n",
        "print(f\"Test-3 missing sentences: {X_test3.isna().sum()}\")\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_model(model, model_name, train_data, test_datasets):\n",
        "    results = []\n",
        "\n",
        "    # Train on the specified training data and evaluate on test sets\n",
        "    X_train_data, y_train_data = train_data\n",
        "\n",
        "    print(f\"\\n===== Training {model_name} on dataset with {len(X_train_data)} samples =====\")\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_data, y_train_data)\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Training time: {train_time:.2f} seconds\")\n",
        "\n",
        "    # Evaluate on test datasets\n",
        "    for dataset_name, (X_test, y_test) in test_datasets.items():\n",
        "        start_time = time.time()\n",
        "        y_pred = model.predict(X_test)\n",
        "        test_time = time.time() - start_time\n",
        "\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        results.append({\n",
        "            'Training Set': f\"{model_name} on {len(X_train_data)} samples\",\n",
        "            'Test Set': dataset_name,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1': f1,\n",
        "            'Accuracy': accuracy,\n",
        "            'Inference Time': f\"{test_time:.2f} sec\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Define training datasets as required\n",
        "train_datasets = {\n",
        "    'TRAIN (All)': (X_train, y_train),\n",
        "    'Train-3': (X_train3, y_train3)\n",
        "}\n",
        "\n",
        "# Define test datasets\n",
        "test_datasets = {\n",
        "    'Test-1': (X_test1, y_test1),\n",
        "    'Test-2': (X_test2, y_test2),\n",
        "    'Test-3': (X_test3, y_test3)\n",
        "}\n",
        "\n",
        "# Print some basic info about the datasets\n",
        "print(\"\\nClass distribution in TRAIN:\")\n",
        "print(y_train.value_counts().to_string())\n",
        "print(\"\\nClass distribution in Train-3:\")\n",
        "print(y_train3.value_counts().to_string())\n",
        "\n",
        "# Method 1: Logistic Regression with TF-IDF\n",
        "# Using parameters that preserve all words/tokens\n",
        "lr_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        max_features=None,    # Don't limit features - use all words\n",
        "        min_df=1,             # Include all terms, even those appearing in just one document\n",
        "        max_df=1.0,           # Include all terms, even extremely common ones\n",
        "        ngram_range=(1, 2),   # Include unigrams and bigrams\n",
        "        sublinear_tf=True     # Apply sublinear tf scaling for better results\n",
        "    )),\n",
        "    ('classifier', LogisticRegression(\n",
        "        C=1.0,                # Regularization strength\n",
        "        max_iter=300,         # Increased iterations for convergence\n",
        "        class_weight='balanced', # Handle class imbalance\n",
        "        solver='saga',        # Efficient for larger datasets\n",
        "        n_jobs=-1,            # Use all CPU cores\n",
        "        random_state=42       # For reproducibility\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Method 2: Random Forest with TF-IDF\n",
        "# Using parameters that preserve all words/tokens\n",
        "rf_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        max_features=None,    # Don't limit features - use all words\n",
        "        min_df=1,             # Include all terms, even those appearing in just one document\n",
        "        max_df=1.0,           # Include all terms, even extremely common ones\n",
        "        ngram_range=(1, 2),   # Include unigrams and bigrams\n",
        "        sublinear_tf=True     # Apply sublinear tf scaling for better results\n",
        "    )),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=100,     # Number of trees\n",
        "        max_depth=None,       # No maximum depth - let trees grow fully\n",
        "        min_samples_split=2,  # Default - split after at least 2 samples\n",
        "        min_samples_leaf=1,   # Default - allow leaf nodes with just 1 sample\n",
        "        max_features='sqrt',  # Consider sqrt(n_features) at each split\n",
        "        class_weight='balanced', # Handle class imbalance\n",
        "        n_jobs=-1,            # Use all CPU cores\n",
        "        random_state=42       # For reproducibility\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Run evaluations for each model on each training dataset\n",
        "all_results = []\n",
        "\n",
        "print(\"\\n==== Evaluating Logistic Regression ====\")\n",
        "for train_name, train_data in train_datasets.items():\n",
        "    lr_results = evaluate_model(lr_pipeline, f\"Logistic Regression on {train_name}\", train_data, test_datasets)\n",
        "    all_results.append(lr_results)\n",
        "\n",
        "print(\"\\n==== Evaluating Random Forest ====\")\n",
        "for train_name, train_data in train_datasets.items():\n",
        "    rf_results = evaluate_model(rf_pipeline, f\"Random Forest on {train_name}\", train_data, test_datasets)\n",
        "    all_results.append(rf_results)\n",
        "\n",
        "# Combine all results\n",
        "combined_results = pd.concat(all_results)\n",
        "print(\"\\n==== All Evaluation Results ====\")\n",
        "print(combined_results.round(4).to_string(index=False))\n",
        "\n",
        "# Summary by training dataset and model\n",
        "print(\"\\n==== Average F1 Scores by Model and Training Set ====\")\n",
        "summary = combined_results.groupby(['Training Set'])['F1'].mean().reset_index()\n",
        "print(summary.round(4).sort_values('F1', ascending=False).to_string(index=False))\n",
        "\n",
        "# Find best combination\n",
        "best_row = combined_results.loc[combined_results['F1'].idxmax()]\n",
        "print(f\"\\nBest performance: {best_row['Training Set']} on {best_row['Test Set']}\")\n",
        "print(f\"Precision: {best_row['Precision']:.4f}, Recall: {best_row['Recall']:.4f}, F1: {best_row['F1']:.4f}, Accuracy: {best_row['Accuracy']:.4f}\")\n"
      ]
    }
  ]
}